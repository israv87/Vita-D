# -*- coding: utf-8 -*-
"""Random Forest VITA-D.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yoQoDgT-EdmhZ4AZBIDvkbnNKfGkk6cR

# Ejemplo clasificación
"""

#!pip install optuna

"""## Librerías"""

# Tratamiento de datos
# ==============================================================================
import numpy as np
import pandas as pd
import statsmodels.api as sm

# Gráficos
# ==============================================================================
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
plt.rcParams['lines.linewidth'] = 1.5
plt.rcParams['font.size'] = 8

# Preprocesado y modelado
# ==============================================================================
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import classification_report
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ParameterGrid
from sklearn.inspection import permutation_importance
from joblib import Parallel, delayed, cpu_count

import optuna

# Configuración warnings
# ==============================================================================
import warnings

"""## Datos
El set de datos Carseats, original del paquete de R ISLR y accesible en Python a través de statsmodels.datasets.get_rdataset, contiene información sobre la venta de sillas infantiles en 400 tiendas distintas. Para cada una de las 400 tiendas se han registrado 11 variables. Se pretende generar un modelo de clasificación que permita predecir si una tienda tiene ventas altas (Sales > 8) o bajas (Sales <= 8) en función de todas las variables disponibles.
"""

# Lectura de datos
# ==============================================================================
dataframe = pd.read_csv(r"/content/BDSeleccionadaV3.csv")
dataframe.head(3)

# Actualizamos la función para categorizar los niveles de Vitamina D
def categorizar_vitamina_d_binario(valor):
    if valor < 15:
        return 0
    else:
        return 1

# Aplicamos la función actualizada a la columna de Vitamina D y creamos una nueva columna
dataframe['CategoriaVitaD'] = dataframe['VitaminaD'].apply(categorizar_vitamina_d_binario)

# No es necesario filtrar el DataFrame, pues todos los valores ahora son incluidos

# Aseguramos que la columna 'CategoriaVitaD' sea de tipo entero
dataframe['CategoriaVitaD'] = dataframe['CategoriaVitaD'].astype(int)

# Muestra las primeras filas del DataFrame modificado para verificar el cambio
print(dataframe.head())

"""## Ajuste del modelo y optimización de hiperparámetros
Se ajusta un árbol de clasificación empleando como variable respuesta ventas_altas y como predictores todas las variables disponibles. Se utilizan en primer lugar los hiperparámetros max_depth=5 y criterion='gini', el resto se dejan por defecto. Después, se aplica el proceso de pruning y se comparan los resultados frente al modelo inicial.
"""

# División de los datos en train y test, excluyendo la columna 'VitaminaD'
# ==============================================================================
X_train, X_test, y_train, y_test = train_test_split(
    dataframe.drop(columns=['CategoriaVitaD', 'VitaminaD','ExposicionMinutos','ExposicionDias'	]),  # Excluir 'VitaminaD' y 'CategoriaVitaD'
    dataframe['CategoriaVitaD'],
    random_state=123
)

# One-hot-encoding de las variables categóricas
# ==============================================================================
# Se identifica el nombre de las columnas numéricas y categóricas
cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.to_list()
numeric_cols = X_train.select_dtypes(include=['float64', 'int']).columns.to_list()

# Se aplica one-hot-encoding solo a las columnas categóricas
preprocessor = ColumnTransformer(
    [('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='if_binary'), cat_cols)],
    remainder='passthrough',
    verbose_feature_names_out=False
).set_output(transform="pandas")

# Aplicación de las transformaciones a los conjuntos de entrenamiento y prueba
X_train_prep = preprocessor.fit_transform(X_train)
X_test_prep = preprocessor.transform(X_test)

X_train_prep.info()

"""## Grid Search basado en out-of-bag score"""

# Grid de hiperparámetros evaluados
# ==============================================================================
param_grid = ParameterGrid(
                {'n_estimators': [150],
                 'max_features': [5, 7, 9],
                 'max_depth'   : [None, 3, 10, 20],
                 'criterion'   : ['gini', 'entropy']
                }
            )


# Loop para ajustar un modelo con cada combinación de hiperparámetros
# ==============================================================================
resultados = {'params': [], 'oob_accuracy': []}

for params in param_grid:

    modelo = RandomForestClassifier(
                oob_score    = True,
                n_jobs       = -1,
                random_state = 123,
                ** params
             )

    modelo.fit(X_train_prep, y_train)

    resultados['params'].append(params)
    resultados['oob_accuracy'].append(modelo.oob_score_)
    print(f"Modelo: {params} \u2713")

# Resultados
# ==============================================================================
resultados = pd.DataFrame(resultados)
resultados = pd.concat([resultados, resultados['params'].apply(pd.Series)], axis=1)
resultados = resultados.sort_values('oob_accuracy', ascending=False)
resultados = resultados.drop(columns='params')
resultados.head(4)

# Mejores hiperparámetros por out-of-bag error
# ==============================================================================
print("--------------------------------------------------")
print("Mejores hiperparámetros encontrados (oob-accuracy)")
print("--------------------------------------------------")
print(resultados.iloc[0,0:])

"""## Grid Search basado en validación cruzada"""

# Grid de hiperparámetros evaluados
# ==============================================================================
param_grid = {
    'n_estimators': [150],
    'max_features': [5, 7, 9],
    'max_depth'   : [None, 3, 10, 20],
    'criterion'   : ['gini', 'entropy']
}

# Búsqueda por grid search con validación cruzada
# ==============================================================================
grid = GridSearchCV(
        estimator  = RandomForestClassifier(random_state = 123),
        param_grid = param_grid,
        scoring    = 'accuracy',
        n_jobs     = cpu_count() - 1,
        cv         = RepeatedKFold(n_splits=5, n_repeats=3, random_state=123),
        refit      = True,
        verbose    = 0,
        return_train_score = True
       )

grid.fit(X=X_train_prep, y=y_train)

# Resultados
# ==============================================================================
resultados = pd.DataFrame(grid.cv_results_)
resultados.filter(regex='(param*|mean_t|std_t)') \
    .drop(columns='params') \
    .sort_values('mean_test_score', ascending = False) \
    .head(4)

# Mejores hiperparámetros encontrados por validación cruzada
# ==============================================================================
print("--------------------------------------------")
print("Mejores hiperparámetros encontrados por (cv)")
print("--------------------------------------------")
print(grid.best_params_, ":", grid.best_score_, grid.scoring)

"""## Predicción y evaluación del modelo"""

# Modelo con los mejores hiperparámetros
# ==============================================================================
modelo_final = grid.best_estimator_
modelo_final

# Error de test del modelo final
# ==============================================================================
predicciones = modelo_final.predict(X=X_test_prep)
predicciones[:10]

mat_confusion = confusion_matrix(y_true=y_test, y_pred=predicciones)
accuracy = accuracy_score(y_true=y_test, y_pred=predicciones, normalize=True)
print("Matriz de confusión")
print("-------------------")
print(mat_confusion)
print("")
print(f"El accuracy de test es: {100 * accuracy} % \n")
fig, ax = plt.subplots(figsize=(3, 3))
ConfusionMatrixDisplay(mat_confusion).plot(ax=ax)

print(
    classification_report(
        y_true = y_test,
        y_pred = predicciones
    )
)

"""## Predicción de probabilidades
 Con .predict_proba(), en lugar de una clasificación, se obtiene la probabilidad con la que el modelo considera que cada observación puede pertenecer a cada una de las clases.
"""

# Predicción de probabilidades
# ==============================================================================
predicciones = modelo_final.predict_proba(X=X_test_prep)
predicciones[:5, :]

# Clasificación empleando la clase de mayor probabilidad
# ==============================================================================
# Ajustando el DataFrame de predicciones para 4 categorías
df_predicciones = pd.DataFrame(data=predicciones, columns=['Suficiencia', 'Insuficiencia'])

# Para clasificar, puedes elegir la categoría con la mayor probabilidad para cada observación
df_predicciones['Clasificacion'] = df_predicciones.idxmax(axis=1)

# Mostrar las primeras filas para verificar el resultado
print(df_predicciones.head())

# Clasificación final empleando un threshold de 0.8 para la clase 1.
# ==============================================================================
# Asumiendo que quieres aplicar el umbral a 'Insuficiencia' y clasificar como '1' si la probabilidad es mayor que 0.8
df_predicciones['clasificacion_custom_0.8'] = np.where(df_predicciones['Insuficiencia'] > 0.8, 1, 0)

# Mostrar las filas de la 5 a la 10 para verificar el resultado
print(df_predicciones.iloc[4:10, :])

"""# Importancia de predictores
## Importancia por pureza de nodos
"""

importancia_predictores = pd.DataFrame(
                            {'predictor': X_train_prep.columns,
                             'importancia': modelo_final.feature_importances_}
                            )
print("Importancia de los predictores en el modelo")
print("-------------------------------------------")
importancia_predictores.sort_values('importancia', ascending=False)

"""##Importancia por permutación"""

importancia = permutation_importance(
                estimator    = modelo_final,
                X            = X_train_prep,
                y            = y_train,
                n_repeats    = 5,
                scoring      = 'neg_root_mean_squared_error',
                n_jobs       = cpu_count() - 1,
                random_state = 123,
             )

# Se almacenan los resultados (media y desviación) en un dataframe
df_importancia = pd.DataFrame(
                    {k: importancia[k] for k in ['importances_mean', 'importances_std']}
                 )
df_importancia['feature'] = X_train_prep.columns
df_importancia.sort_values('importances_mean', ascending=False)

# Gráfico
# ==============================================================================
fig, ax = plt.subplots(figsize=(3.5, 4))
df_importancia = df_importancia.sort_values('importances_mean', ascending=True)
ax.barh(
    df_importancia['feature'],
    df_importancia['importances_mean'],
    xerr=df_importancia['importances_std'],
    align='center',
    alpha=0
)
ax.plot(
    df_importancia['importances_mean'],
    df_importancia['feature'],
    marker="D",
    linestyle="",
    alpha=0.8,
    color="r"
)
ax.set_title('Importancia de los predictores (train)')
ax.set_xlabel('Incremento del error tras la permutación');